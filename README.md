# Loan_Defaulter_XGBoost:
Developed an XGBoost model to classify Loan Defaulters with ~70% Recall value and identified top Risk as well as Capacity-based profiling features impacting write-offs by estimating Gain-based Feature Importance. 


**Problem Statement**
Financial institutions face significant challenges in assessing credit risk and predicting loan defaults. With default rates affecting both profitability and regulatory compliance, there's a critical need for accurate prediction models.

**Business Challenge**
High costs associated with loan defaults
Need for early risk identification
Regulatory requirements for risk assessment
Resource allocation for debt collection
Portfolio risk management

Technical Objectives

Build ML model to predict credit default probability
Identify key risk indicators
Create actionable risk scoring system
Enable real-time risk assessment
Develop interpretable model insights

Data Context

Dataset Size: 307,511 credit applications Ã— 121 features
Features Categories:

Demographic: age, gender, education, employment
Financial: income, credit amount, annuity
External Scores: three normalized risk ratings
Documentation: 21 document flags
Regional: population, rating, housing statistics
Social: family, social circle information


Target Variable: Binary default status (1 = Default, 0 = Non-Default)

Success Metrics

Model Performance:

Target Recall > 70%
Risk Segmentation Effectiveness
Feature Importance Insights
Default Capture Rate in Top Risk Buckets



Implementation Details
Data Preprocessing

Missing Value Analysis

pythonCopy# Calculate missing rates
df_missing_rate = (df.isnull().sum()*100/len(df)).reset_index()
df_missing_rate = df_missing_rate.rename(columns={
    'index': 'variable_name', 
    '0': 'missing_rate'
})

Data Split

pythonCopy# Train-test split with stratification
X = df.drop(columns='TARGET', axis=1)
y = df.TARGET
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.3, 
    random_state=42, 
    stratify=y
)
Feature Engineering

Weight of Evidence (WOE) Encoding

pythonCopy# Calculate WOE for categorical variables
df_train["bads"] = 1-df_train["TARGET"]
total_good_events = df_train['TARGET'].sum()
total_bad_events = df_train['bads'].sum()

for col_ in categorical_var_list:
    # Group by category
    df_grouped = df_train.groupby(by=col_).agg({
        'TARGET': 'sum', 
        'bads': 'sum'
    }).reset_index()
    
    # Calculate proportions
    df_grouped['perc_goods'] = df_grouped.TARGET/total_good_events
    df_grouped['perc_bads'] = df_grouped.bads/total_bad_events
    
    # Calculate WOE and IV
    df_grouped['woe'] = np.log(df_grouped['perc_goods']/df_grouped['perc_bads'])
    df_grouped['IV'] = (df_grouped['perc_goods'] - df_grouped['perc_bads']) * df_grouped['woe']
Model Development

XGBoost Configuration

pythonCopybest_params = {
    'max_depth': 6,
    'min_child_weight': 1,
    'reg_alpha': 0.01,
    'reg_lambda': 0.1,
    'colsample_bytree': 0.5,
    'scale_pos_weight': 11,
    'n_estimators': 60
}

model_xgb = XGBClassifier(**best_params)
model_xgb.fit(df_train[indep_vars], df_train.TARGET)

Feature Importance (Gain-based)

pythonCopy# Calculate feature importance
feature_imp_dict = model_xgb.get_booster().get_score(importance_type='gain')
df_feature_imp = pd.DataFrame({
    'feature': list(feature_imp_dict.keys()),
    'importance': list(feature_imp_dict.values())
})
df_feature_imp['relative_importance'] = (
    df_feature_imp.importance/df_feature_imp.importance.sum() * 100
)

Probability Bucket Analysis

pythonCopy# Create probability buckets
df_train['pred_prob_bucket'] = pd.qcut(
    df_train['predicted_prob_of_default'], 10
)
df_train['volume'] = 1

# Calculate metrics by bucket
df_grouped = df_train.groupby('pred_prob_bucket').agg({
    'volume': 'sum',
    'TARGET': 'sum',
    'predicted_prob_of_default': 'mean'
}).reset_index()

df_grouped['default_rate'] = df_grouped.TARGET/df_grouped.volume * 100
df_grouped['capture_rate'] = df_grouped.TARGET/df_grouped.TARGET.sum() * 100
Model Performance
Training Data Performance

Recall: 0.76
Precision: 0.21
F1 Score: 0.33

Testing Data Performance

Recall: 0.62
Precision: 0.17
F1 Score: 0.27

Top 10 Predictive Features

External Score 3 (4.9%)
Education Level (4.0%)
External Score 2 (3.8%)
Gender (3.4%)
Work Phone Verification (3.1%)
Document 3 (2.3%)
Region Rating (2.2%)
External Score 1 (1.8%)
Document 6 (1.6%)
Age (Days) (1.6%)

Risk Bucket Analysis

Highest Risk Bucket (0.701-0.97):

Default Rate: 32.5%
Capture Rate: 40.3%
Volume: 21,526 applications


Lowest Risk Bucket (0.0005-0.112):

Default Rate: 0.06%
Capture Rate: 0.07%
Volume: 21,526 applications



Visualizations
Feature Importance Plot
pythonCopyplt.figure(figsize=(10, 6))
plt.barh(y=top_features['feature_name'][:10], 
         width=top_features['relative_importance'][:10],
         color='#3B82F6')
plt.title('Top 10 Features Impact on Default Prediction (%)')
plt.xlabel('Impact %')
plt.gca().spines[['top', 'right']].set_visible(False)
plt.tight_layout()
plt.show()
Default Rate Analysis Plot
pythonCopyplt.figure(figsize=(12, 6))
ax1 = plt.gca()
ax2 = ax1.twinx()

# Plot default rate bars
bars = ax1.bar(range(len(df_grouped)), 
               df_grouped['default_rate'], 
               color='blue', 
               alpha=0.5)
ax1.set_xlabel('Probability Buckets')
ax1.set_ylabel('Default Rate (%)', color='blue')

# Plot capture rate line
line = ax2.plot(range(len(df_grouped)), 
                df_grouped['capture_rate'], 
                color='red', 
                linewidth=2)
ax2.set_ylabel('Defaulter Capture Rate (%)', color='red')

plt.title('Default Rate and Capture Rate by Probability Bucket')
plt.tight_layout()
plt.show()
Business Impact
Risk Assessment

Early identification of high-risk applications
40.3% of defaults captured in highest risk segment
541:1 risk discrimination ratio between highest and lowest buckets

Decision Support

Probability scores for risk-based pricing
Feature importance for application review focus
Risk segmentation for portfolio management

Operational Efficiency

Automated risk assessment
Standardized evaluation criteria
Data-driven decision making

Dependencies

Python 3.7+
pandas
numpy
scikit-learn
xgboost
matplotlib
seaborn

Future Improvements

Feature engineering for temporal variables
Cross-validation for model stability
Hyperparameter optimization using Bayesian methods
Model interpretability using SHAP values
Regular model retraining pipeline
Additional data source integration
